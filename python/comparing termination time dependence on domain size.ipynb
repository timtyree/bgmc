{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cd2399",
   "metadata": {},
   "source": [
    "# comparing termination time dependence on domain size\n",
    "Tim Tyree<br>\n",
    "6.10.2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35752fab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T20:12:53.876Z"
    }
   },
   "outputs": [],
   "source": [
    "from lib.my_initialization import *\n",
    "parse_fortranic_tip_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02e694",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T20:10:21.166Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([1]*4)\n",
    "plt.close()\n",
    "#reset matplotlib\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c874eb5",
   "metadata": {},
   "source": [
    "# define module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7365b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T20:10:21.954Z"
    }
   },
   "outputs": [],
   "source": [
    "def gener_conditional_termination_times(log_folder_parquet,\n",
    "                                        N0=2,\n",
    "                                        seed = 42,\n",
    "                                        nv_once_every=10,\n",
    "                                        constrain_to_N0=True,\n",
    "                                        return_particle_numbers=False,\n",
    "                                        **kwargs):\n",
    "    \"\"\"\n",
    "    Example Usage:\n",
    "t_lst,nv_lst = gener_conditional_termination_times(log_folder_parquet,N0=2,seed=42,nv_once_every=10,\n",
    "                                        constrain_to_N0=True,return_particle_numbers=False)#,**kwargs)\n",
    "    \"\"\"\n",
    "    t_lst=[]\n",
    "    nv_lst=[]\n",
    "    np.random.seed(seed)\n",
    "    for fn in os.listdir(log_folder_parquet):\n",
    "        try:\n",
    "            trial_num = eval(fn.split('=')[1])\n",
    "            # load tip position data\n",
    "            g=load_parquet_by_trial_num(trial_num=trial_num,folder_parquet=log_folder_parquet)\n",
    "            n_series = g.groupby(by='t')['n'].min()\n",
    "            t0_values = n_series[n_series==N0].index.values\n",
    "            tf = n_series.index.values.max()\n",
    "            t_values = tf-t0_values\n",
    "            #record\n",
    "            t_lst.extend(t_values)\n",
    "            if len(t_values)>0 and return_particle_numbers:\n",
    "                nv = g.groupby('t')['n'].mean()[::nv_once_every] #once every 10ms\n",
    "                nv = nv[nv%2==0]\n",
    "                if constrain_to_N0:\n",
    "                    t0 = np.random.choice(t0_values)\n",
    "                    nv = nv[nv.index.values>=t0].copy()\n",
    "                nv_lst.extend(nv)\n",
    "        except IndexError as e:\n",
    "            pass\n",
    "    return t_lst,nv_lst\n",
    "        #print(f\"IndexError for {fn}: {e}\")\n",
    "# n_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3f14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d703b676",
   "metadata": {},
   "source": [
    "# plot termination versus domain size for the full models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c13060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbf659",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T20:10:23.444Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = f\"{nb_dir}/Data/full_results\"\n",
    "os.path.exists(data_folder)\n",
    "\n",
    "\n",
    "\n",
    "Nbar_dir = os.path.join(data_folder,'Nbar_vs_A_full.csv')\n",
    "taubar_dir = os.path.join(data_folder,'taubar_vs_A_full.csv')\n",
    "df_Nbar = pd.read_csv(Nbar_dir)\n",
    "df_taubar = pd.read_csv(taubar_dir)\n",
    "# df_taubar\n",
    "# df_Nbar\n",
    "\n",
    "\n",
    "\n",
    "from scipy.optimize import dual_annealing\n",
    "#only magnitude is to be varied\n",
    "def comp_ss_err(x,*args):\n",
    "    A_values,tau_values,powr,navg = args\n",
    "    tau_pred_values = x * (2/A_values)**(powr) * np.exp(A_values*navg*powr/2)\n",
    "    se = (tau_values-tau_pred_values) ** 2\n",
    "    sse = np.sum(se)\n",
    "    #return sse\n",
    "    #smpe = np.sum(np.log(tau_pred_values/tau_values))\n",
    "    smpe = np.mean( se / tau_values**2 )\n",
    "    return smpe\n",
    "\n",
    "\n",
    "xfk = df_taubar[df_taubar['model_name']=='FK']['A'].values\n",
    "yfk = df_taubar[df_taubar['model_name']=='FK']['taubar'].values\n",
    "xlr = df_taubar[df_taubar['model_name']=='LR']['A'].values\n",
    "ylr = df_taubar[df_taubar['model_name']=='LR']['taubar'].values\n",
    "\n",
    "\n",
    "# Mp = dict_fit_birth_fk['M']\n",
    "# nup = dict_fit_birth_fk['m']\n",
    "# Mm = dict_fit_death_fk['M']\n",
    "# num = dict_fit_death_fk['m']\n",
    "model_name='Fenton-Karma'\n",
    "A_values = xfk\n",
    "tau_values = yfk\n",
    "\n",
    "\n",
    "# Mp = dict_fit_birth_lr['M']\n",
    "# nup = dict_fit_birth_lr['m']\n",
    "# Mm = dict_fit_death_lr['M']\n",
    "# num = dict_fit_death_lr['m']\n",
    "# model_name='Luo-Rudy'\n",
    "# A_values = xlr\n",
    "# tau_values = ylr\n",
    "\n",
    "# #simulated annealing fit \n",
    "# powr = num - nup\n",
    "# navg = (Mp/Mm) ** (1/powr)\n",
    "# tau_pred_foo = lambda x: x * (2/A_values)**(powr) * np.exp(A_values*navg*powr/2)\n",
    "\n",
    "\n",
    "#fk\n",
    "# mag=2.033588801144e+00 ± 0.000000000000e+00\n",
    "mag=2.0335888011441683\n",
    "# res_again.x=array([2.0335888])\n",
    "# # magfk=1.566390007127e+00 #± 3.762187006373e-09 #1e3 epochs\n",
    "# # magfk=1.566390008379e+00 #± 5.014229476075e-09 #1e4 epochs\n",
    "magfk = mag\n",
    "\n",
    "maglr=9.319636219057e-01 #± 3.981174279133e-10 #1e3 epochs\n",
    "maglr=9.319636257483e-01 #± 3.444428298138e-09 #1e4 epochs\n",
    "maglr = mag\n",
    "\n",
    "\n",
    "# Mp = dict_fit_birth_fk['M']\n",
    "# nup = dict_fit_birth_fk['m']\n",
    "# Mm = dict_fit_death_fk['M']\n",
    "# num = dict_fit_death_fk['m']\n",
    "model_name='Fenton-Karma'\n",
    "# powr = num - nup\n",
    "# navg = (Mp/Mm) ** (1/powr)\n",
    "# yfk_pred = magfk * (2/xfk)**(powr) * np.exp(xfk*navg*powr/2)\n",
    "# powrfk = float(powr)\n",
    "# navgfk = float(navg)\n",
    "# tau_pred_foofk = lambda x: float(magfk) * (2/x)**float(powrfk) * np.exp(x*float(navgfk*powrfk/2))\n",
    "\n",
    "\n",
    "# Mp = dict_fit_birth_lr['M']\n",
    "# nup = dict_fit_birth_lr['m']\n",
    "# Mm = dict_fit_death_lr['M']\n",
    "# num = dict_fit_death_lr['m']\n",
    "model_name='Luo-Rudy'\n",
    "# powr = num - nup\n",
    "# navg = (Mp/Mm) ** (1/powr)\n",
    "# ylr_pred = maglr * (2/xlr)**(powr) * np.exp(xlr*navg*powr/2)\n",
    "# tau_pred_foolr = lambda x: float(maglr) * (2/x)**float(powr) * np.exp(x*float(navg*powr/2))\n",
    "\n",
    "\n",
    "#plot the simulated annealing fits\n",
    "figsize=(3,4)\n",
    "fontsize=15\n",
    "alpha=0.7\n",
    "fig,ax=plt.subplots(figsize=figsize)\n",
    "ax.scatter(xfk,yfk,c='C0',alpha=alpha,label='Fenton-Karma')\n",
    "ax.scatter(xlr,ylr,c='C1',alpha=alpha,label='Luo-Rudy')\n",
    "# # ax.scatter(xfk,yfk_pred,c='k',alpha=alpha,marker='+')#,label='Fenton-Karma')\n",
    "# # ax.scatter(xlr,ylr_pred,c='k',alpha=alpha,marker='+')#,label='Luo-Rudy')\n",
    "# # ax.scatter([A_fk],[tau_fk],marker='o',edgecolor='k',c='C0',#'k',\n",
    "# #            alpha=1,label='Fenton-Karma')\n",
    "# # ax.scatter([A_lr],[tau_lr],marker='o',edgecolor='k',c='C1',#c='k',\n",
    "# #            alpha=1,label='Luo-Rudy')\n",
    "# # xv=np.linspace(0,40,10)\n",
    "# # ax.plot(xv,taubar0fk*np.exp(xv/A0fk),':',color='gray')#,'k--',lw=2)\n",
    "# # xv=np.linspace(0,80,10)\n",
    "# # ax.plot(xv,taubar0lr*np.exp(xv/A0lr),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,40,10)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foofk(xv),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foolr(xv),':',color='gray')#,'k--',lw=2)\n",
    "\n",
    "ax.set_xticks([0,50,100,150])\n",
    "\n",
    "# format_plot(ax=ax,xlabel=r'$a$ (cm$^2/$s)',ylabel=r'$\\tau$  (s)',fontsize=16)\n",
    "format_plot(ax=ax,xlabel=r'$A$ (cm$^2$)',ylabel=r'$\\langle\\tau\\rangle$  (s)',fontsize=16)\n",
    "# ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='both')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='minor',length=3)\n",
    "# ax.set_ylim([1e-2,100])\n",
    "# # AddLegend(ax=ax,xy=(-0.12,0.240),fontsize=fontsize-3)\n",
    "leg = ax.legend(loc='upper right',fontsize=fontsize-2)#,alpha=1.)#,frameon=False,ncol=1)\n",
    "# leg.set_alpha(1.)\n",
    "# , edgecolor=\"black\")\n",
    "leg.get_frame().set_alpha(None)\n",
    "# leg.get_frame().set_facecolor((1, 1, 1, 1))\n",
    "# leg = ax.legend(loc='lower right',fontsize=fontsize-3.,frameon=False,ncol=1)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "# ax.set_ylim([0.31392551380514033, 4e9])\n",
    "ax.set_xlim([0.,100])\n",
    "ax.set_ylim([0.13850549065122997,12000])\n",
    "# AddLege nd(ax=ax,xy=(0.05,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.7,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.4,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.1,0.25),fontsize=fontsize-3)\n",
    "# ax.tick_params(axis='y', which='minor', left=True)\n",
    "# plt.minorticks_on()\n",
    "plt.show()\n",
    "ax.axis()\n",
    "#og src: http://localhost:8888/notebooks/Simulating%20the%20paired%20birth%20death%20process.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050fdc6",
   "metadata": {},
   "source": [
    "# DONE: find out which L to use for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b66927",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T20:10:24.239Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# L_values_fk = np.around(np.sqrt(xfk),5)\n",
    "# L_values_lr = np.around(np.sqrt(xlr),5)\n",
    "L_values_fk = np.array([4.5  , 4.75 , 5.   , 5.25 , 5.625, 6.25 ])\n",
    "L_values_lr = np.array([5.  , 6.25, 7.5 , 8.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbecffb",
   "metadata": {},
   "source": [
    "# DONE: dev next run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834a252",
   "metadata": {},
   "source": [
    "# DONE: time permitting, eat something."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d433c",
   "metadata": {},
   "source": [
    "# DONE: download the results and name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9aae8",
   "metadata": {},
   "source": [
    "# DONE: parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca0b6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T20:10:28.887Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: plot against particle model (run #46, N=2000 sims per data point)\n",
    "# taul_dir = f\"{nb_dir}/data/osg_output/run_46_tau_vs_L.csv\"\n",
    "# taul_dir = f\"{nb_dir}/data/osg_output/run_48_tau_vs_L.csv\"\n",
    "taul_dir = f\"{nb_dir}/data/osg_output/run_48_tau_vs_L_quantile_.99.csv\"\n",
    "df_taul = pd.read_csv(taul_dir)\n",
    "df_taul['A'] = df_taul['L'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bfa86",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T20:10:29.719Z"
    }
   },
   "outputs": [],
   "source": [
    "figsize=(3,4)\n",
    "fontsize=15\n",
    "alpha=0.7\n",
    "max_num_points_to_show = 7\n",
    "fig,ax=plt.subplots(figsize=figsize)\n",
    "# color_lst = ['C0','C1']\n",
    "color_lst = ['k','r']\n",
    "#plot particle model results\n",
    "for i,(D,g) in enumerate(df_taul.groupby('D')):\n",
    "    color = f\"C{i}\"\n",
    "    color = color_lst[i]\n",
    "#     g.plot.scatter(x='A',y='tau',color=color,ax=ax,marker='+')\n",
    "#     ax.errorbar(g['A'], g['tau'], yerr=g['Delta_tau'],color=color,marker='+')\n",
    "    ax.errorbar(g['A'].head(max_num_points_to_show), g['tau'].head(max_num_points_to_show), yerr=g['Delta_tau'].head(max_num_points_to_show),color=color,marker='+')\n",
    "\n",
    "#plot full model results\n",
    "ax.scatter(xfk,yfk,c='C0',alpha=alpha,label='Fenton-Karma')\n",
    "ax.scatter(xlr,ylr,c='C1',alpha=alpha,label='Luo-Rudy')\n",
    "#plot the simulated annealing fits\n",
    "# # ax.scatter(xfk,yfk_pred,c='k',alpha=alpha,marker='+')#,label='Fenton-Karma')\n",
    "# # ax.scatter(xlr,ylr_pred,c='k',alpha=alpha,marker='+')#,label='Luo-Rudy')\n",
    "# # ax.scatter([A_fk],[tau_fk],marker='o',edgecolor='k',c='C0',#'k',\n",
    "# #            alpha=1,label='Fenton-Karma')\n",
    "# # ax.scatter([A_lr],[tau_lr],marker='o',edgecolor='k',c='C1',#c='k',\n",
    "# #            alpha=1,label='Luo-Rudy')\n",
    "# # xv=np.linspace(0,40,10)\n",
    "# # ax.plot(xv,taubar0fk*np.exp(xv/A0fk),':',color='gray')#,'k--',lw=2)\n",
    "# # xv=np.linspace(0,80,10)\n",
    "# # ax.plot(xv,taubar0lr*np.exp(xv/A0lr),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,40,10)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foofk(xv),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foolr(xv),':',color='gray')#,'k--',lw=2)\n",
    "ax.set_xticks([0,50,100,150])\n",
    "# format_plot(ax=ax,xlabel=r'$a$ (cm$^2/$s)',ylabel=r'$\\tau$  (s)',fontsize=16)\n",
    "format_plot(ax=ax,xlabel=r'$A$ (cm$^2$)',ylabel=r'$\\langle\\tau\\rangle$  (s)',fontsize=16)\n",
    "# ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='both')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='minor',length=3)\n",
    "# ax.set_ylim([1e-2,100])\n",
    "# # AddLegend(ax=ax,xy=(-0.12,0.240),fontsize=fontsize-3)\n",
    "leg = ax.legend(loc='upper right',fontsize=fontsize-2)#,alpha=1.)#,frameon=False,ncol=1)\n",
    "# leg.set_alpha(1.)\n",
    "# , edgecolor=\"black\")\n",
    "leg.get_frame().set_alpha(None)\n",
    "# leg.get_frame().set_facecolor((1, 1, 1, 1))\n",
    "# leg = ax.legend(loc='lower right',fontsize=fontsize-3.,frameon=False,ncol=1)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "# ax.set_ylim([0.31392551380514033, 4e9])\n",
    "ax.set_xlim([0.,120])\n",
    "ax.set_ylim([0.13850549065122997,12000])\n",
    "# AddLege nd(ax=ax,xy=(0.05,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.7,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.4,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.1,0.25),fontsize=fontsize-3)\n",
    "# ax.tick_params(axis='y', which='minor', left=True)\n",
    "# plt.minorticks_on()\n",
    "plt.show()\n",
    "ax.axis()\n",
    "#og src: http://localhost:8888/notebooks/Simulating%20the%20paired%20birth%20death%20process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e32a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(3,4)\n",
    "fontsize=15\n",
    "alpha=0.7\n",
    "max_num_points_to_show = 7\n",
    "fig,ax=plt.subplots(figsize=figsize)\n",
    "# color_lst = ['C0','C1']\n",
    "color_lst = ['k','r']\n",
    "#plot particle model results\n",
    "for i,(D,g) in enumerate(df_taul.groupby('D')):\n",
    "    color = f\"C{i}\"\n",
    "    color = color_lst[i]\n",
    "#     g.plot.scatter(x='A',y='tau',color=color,ax=ax,marker='+')\n",
    "#     ax.errorbar(g['A'], g['tau'], yerr=g['Delta_tau'],color=color,marker='+')\n",
    "    ax.errorbar(g['A'].head(max_num_points_to_show), g['tau'].head(max_num_points_to_show), yerr=g['Delta_tau'].head(max_num_points_to_show),color=color,marker='+')\n",
    "\n",
    "#plot full model results\n",
    "ax.scatter(xfk,yfk,c='C0',alpha=alpha,label='Fenton-Karma')\n",
    "# ax.scatter(xlr,ylr,c='C1',alpha=alpha,label='Luo-Rudy')\n",
    "#plot the simulated annealing fits\n",
    "# # ax.scatter(xfk,yfk_pred,c='k',alpha=alpha,marker='+')#,label='Fenton-Karma')\n",
    "# # ax.scatter(xlr,ylr_pred,c='k',alpha=alpha,marker='+')#,label='Luo-Rudy')\n",
    "# # ax.scatter([A_fk],[tau_fk],marker='o',edgecolor='k',c='C0',#'k',\n",
    "# #            alpha=1,label='Fenton-Karma')\n",
    "# # ax.scatter([A_lr],[tau_lr],marker='o',edgecolor='k',c='C1',#c='k',\n",
    "# #            alpha=1,label='Luo-Rudy')\n",
    "# # xv=np.linspace(0,40,10)\n",
    "# # ax.plot(xv,taubar0fk*np.exp(xv/A0fk),':',color='gray')#,'k--',lw=2)\n",
    "# # xv=np.linspace(0,80,10)\n",
    "# # ax.plot(xv,taubar0lr*np.exp(xv/A0lr),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,40,10)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foofk(xv),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foolr(xv),':',color='gray')#,'k--',lw=2)\n",
    "ax.set_xticks([0,50,100,150])\n",
    "# format_plot(ax=ax,xlabel=r'$a$ (cm$^2/$s)',ylabel=r'$\\tau$  (s)',fontsize=16)\n",
    "format_plot(ax=ax,xlabel=r'$A$ (cm$^2$)',ylabel=r'$\\langle\\tau\\rangle$  (s)',fontsize=16)\n",
    "# ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='both')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='minor',length=3)\n",
    "# ax.set_ylim([1e-2,100])\n",
    "# # AddLegend(ax=ax,xy=(-0.12,0.240),fontsize=fontsize-3)\n",
    "leg = ax.legend(loc='upper right',fontsize=fontsize-2)#,alpha=1.)#,frameon=False,ncol=1)\n",
    "# leg.set_alpha(1.)\n",
    "# , edgecolor=\"black\")\n",
    "leg.get_frame().set_alpha(None)\n",
    "# leg.get_frame().set_facecolor((1, 1, 1, 1))\n",
    "# leg = ax.legend(loc='lower right',fontsize=fontsize-3.,frameon=False,ncol=1)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "# ax.set_ylim([0.31392551380514033, 4e9])\n",
    "ax.set_xlim([0.,50])\n",
    "# ax.set_xlim([0.,120])\n",
    "# ax.set_ylim([0.13850549065122997,12000])\n",
    "ax.set_ylim([0.13850549065122997,12000])\n",
    "# AddLege nd(ax=ax,xy=(0.05,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.7,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.4,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.1,0.25),fontsize=fontsize-3)\n",
    "# ax.tick_params(axis='y', which='minor', left=True)\n",
    "# plt.minorticks_on()\n",
    "plt.show()\n",
    "ax.axis()\n",
    "#og src: http://localhost:8888/notebooks/Simulating%20the%20paired%20birth%20death%20process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bea39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69db4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdd112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704a93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf511df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f79117b4",
   "metadata": {},
   "source": [
    "## Nota bene: ibid is comparing apples to oranges\n",
    "$\\langle \\tau \\rangle \\neq \\langle \\tau(N=2) \\rangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3c95f",
   "metadata": {},
   "source": [
    "# TODO: recompute tau from full models but conditioned on $N_0=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69100cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:33.179817Z",
     "start_time": "2025-07-11T00:36:33.175931Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fn(inptfldr,fn1 = 'tippos_per_c_001',fn2 = 'tippos_per_001'):\n",
    "    it = os.listdir(inptfldr)\n",
    "    if fn1 in it:\n",
    "        return fn1\n",
    "    elif fn2 in it:\n",
    "        return fn2\n",
    "    raise Exception(f\"Warning: fn not found in {inptfldr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c9e454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:33.804490Z",
     "start_time": "2025-07-11T00:36:33.788464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_values_lr=array([ 25.    ,  39.0625,  56.25  ,  76.5625, 100.    ])\n",
      "A_values_fk=array([14.0625  , 25.      , 39.0625  , 50.055625, 76.5625  ])\n",
      "L_values_lr=array([ 5.  ,  6.25,  7.5 ,  8.75, 10.  ])\n",
      "L_values_fk=array([3.75 , 5.   , 6.25 , 7.075, 8.75 ])\n"
     ]
    }
   ],
   "source": [
    "#load fortranic tip positions\n",
    "printing=True\n",
    "nb_dir_ = '/Users/timothytyree/Documents/GitHub/care/notebooks'\n",
    "# folder=f\"{nb_dir_}/Data/from_wjr/positions_fk/200x200\"\n",
    "# folder=f\"{nb_dir_}/Data/from_wjr/positions_fk/\"\n",
    "# fn = 'tippos_per_c_001'\n",
    "folder=f\"{nb_dir_}/Data/from_wjr/positions_lr/\"\n",
    "# fn = 'tippos_per_001'\n",
    "fldr_lst = sorted([x for x in os.listdir(folder) if x[0]!='.'])\n",
    "width_lst = [x.split('x')[0] for x in fldr_lst]\n",
    "height_lst = [x.split('x')[1] for x in fldr_lst]\n",
    "parea_lst = [eval(x.replace('x','*')) for x in fldr_lst]\n",
    "lfactor = 5/200 #cm/pixel\n",
    "A_values = (lfactor **2) * np.array(parea_lst)\n",
    "A_values_lr = A_values.copy()\n",
    "L_values_lr = np.sqrt(A_values_lr)\n",
    "print(f\"{A_values_lr=}\")\n",
    "\n",
    "folder=f\"{nb_dir_}/Data/from_wjr/positions_fk/\"\n",
    "# fn = 'tippos_per_c_001'\n",
    "# folder=f\"{nb_dir_}/Data/from_wjr/positions_lr/200x200\"\n",
    "# fn = 'tippos_per_001'\n",
    "fldr_lst = sorted([x for x in os.listdir(folder) if x[0]!='.'])\n",
    "parea_lst = [eval(x.replace('x','*')) for x in fldr_lst]\n",
    "# parea_lst = []\n",
    "# for x in fldr_lst:\n",
    "#     try:\n",
    "#         parea_lst.append(eval(x.replace('x','*')))\n",
    "#     except NameError as e: \n",
    "#        pass\n",
    "lfactor = 5/200 #cm/pixel\n",
    "A_values = (lfactor **2) * np.array(parea_lst)\n",
    "L_values = np.sqrt(A_values)\n",
    "A_values_fk = A_values.copy()\n",
    "L_values_fk = np.sqrt(A_values_fk)\n",
    "print(f\"{A_values_fk=}\")\n",
    "\n",
    "print(f\"{L_values_lr=}\")\n",
    "print(f\"{L_values_fk=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb84c2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:34.446861Z",
     "start_time": "2025-07-11T00:36:34.444295Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_taul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176a7b65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:35.118452Z",
     "start_time": "2025-07-11T00:36:34.898710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mpositions\u001b[m\u001b[m\r\n",
      "tippos_per_c_001\r\n",
      "tippos_per_c_001_annihilation_msr.csv\r\n",
      "tippos_per_c_001_annihilation_range_timeseries.pkl\r\n",
      "tippos_per_c_001_creation_range_timeseries.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/150x150\n",
    "#TODO: get tippos_per_c_001 for each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e128e3d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:35.511137Z",
     "start_time": "2025-07-11T00:36:35.492511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/150x150/tippos_per_c_001\n",
      "/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/200x200/tippos_per_c_001\n",
      "/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/250x250/tippos_per_001\n",
      "Warning: fn not found in /Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/283x283\n",
      "Warning: fn not found in /Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/350x350\n"
     ]
    }
   ],
   "source": [
    "use_save_df_to_parquet_by=True\n",
    "use_save_df_to_parquet_by=False\n",
    "for i,(L,fldr) in enumerate(zip(L_values,fldr_lst)):\n",
    "    inptfldr = os.path.join(folder,fldr)\n",
    "    try:\n",
    "        fn = get_fn(inptfldr,fn1 = 'tippos_per_c_001',fn2 = 'tippos_per_001')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        fn = None\n",
    "    if fn is not None:\n",
    "        input_dir = os.path.join(folder,fldr,fn)\n",
    "        log_folder_parquet=f'{folder}/{fldr}/positions'\n",
    "        print(input_dir)\n",
    "    #     print(os.listdir())\n",
    "        assert os.path.exists(input_dir)\n",
    "        if use_save_df_to_parquet_by or not os.path.exists(log_folder_parquet):\n",
    "            df_log=parse_fortranic_tip_pos(input_dir)\n",
    "            assert df_log.shape[0]>0\n",
    "            #partition df_log into a folder of tip logs\n",
    "            save_df_to_parquet_by(df_log,log_folder_parquet,by='trial_num',compression='snappy',index=None)\n",
    "            print(f\"saved to spiral tip positions to {log_folder_parquet=}\")\n",
    "    #         #determine width and height of the computational domain input the discretization\n",
    "    #         width,height=df_log.describe().loc['max'][['x','y']].values.T\n",
    "            del df_log\n",
    "    #     else:\n",
    "    #         width = width_lst[i]\n",
    "    #         height = height_lst[i]\n",
    "    #determine width and height of the computational domain input the discretization\n",
    "        width = width_lst[i]\n",
    "        height = height_lst[i]\n",
    "    #     width=200\n",
    "    #     height=200\n",
    "    width,height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bf0db66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:40.915540Z",
     "start_time": "2025-07-11T00:36:36.014621Z"
    }
   },
   "outputs": [],
   "source": [
    "N0=2\n",
    "constrain_to_N0=True\n",
    "t_lst=[]\n",
    "nv_lst=[]\n",
    "np.random.seed(42)\n",
    "for fn in os.listdir(log_folder_parquet):\n",
    "    try:\n",
    "        trial_num = eval(fn.split('=')[1])\n",
    "        # load tip position data\n",
    "        g=load_parquet_by_trial_num(trial_num=trial_num,folder_parquet=log_folder_parquet)\n",
    "        n_series = g.groupby(by='t')['n'].min()\n",
    "        t0_values = n_series[n_series==N0].index.values\n",
    "        tf = n_series.index.values.max()\n",
    "        t_values = tf-t0_values\n",
    "        if len(t_values)>0:\n",
    "            #record\n",
    "            t_lst.extend(t_values)\n",
    "    #         nv = g.groupby('t')['n'].mean()[::100] #once every 100ms\n",
    "            nv = g.groupby('t')['n'].mean()[::10] #once every 10ms\n",
    "            nv = nv[nv%2==0]\n",
    "            if constrain_to_N0:\n",
    "                t0 = np.random.choice(t0_values)\n",
    "                nv = nv[nv.index.values>=t0].copy()\n",
    "            nv_lst.extend(nv)\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "        #print(f\"IndexError for {fn}: {e}\")\n",
    "# n_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3e3a8c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:41.188352Z",
     "start_time": "2025-07-11T00:36:40.916624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meant=144830.78007249834,Delta_meant=1908.5014454263894,num_obs=20966,p_normal=0.2812118029723241\n",
      "median: 98089.5 (IQR: 27904.25-215754.75, N=20966)\n"
     ]
    }
   ],
   "source": [
    "#print summary stats for distribution of termination times\n",
    "tau_values_fk = np.array(t_lst)\n",
    "tau_values = tau_values_fk.copy()\n",
    "meant,Delta_meant,num_obs,p_normal=comp_mean_bootstrap_uncertainty(tau_values)\n",
    "print(f\"{meant=},{Delta_meant=},{num_obs=},{p_normal=}\")\n",
    "median = np.median(tau_values)\n",
    "q25 = np.quantile(tau_values,0.25)\n",
    "q75 = np.quantile(tau_values,0.75)\n",
    "print(f\"median: {median} (IQR: {q25}-{q75}, N={tau_values.shape[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947580e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c462caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f64890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd340319",
   "metadata": {},
   "source": [
    "# dress up the plot to look nice and pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b112ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:36:45.863534Z",
     "start_time": "2025-07-11T00:36:45.860331Z"
    }
   },
   "outputs": [],
   "source": [
    "#GOAL: compute tau(N0=2) for the full models at each domain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87537419",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:25:30.919Z"
    }
   },
   "outputs": [],
   "source": [
    "use_save_df_to_parquet_by = False\n",
    "# use_save_df_to_parquet_by = True  #<<< makes it have a super long run time... but once you start it you gotta let it finish...\n",
    "#for each full model\n",
    "folder = '/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/'\n",
    "# folder = '/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_lr/'\n",
    "for fldr,fn_lst in [(x,os.listdir(os.path.join(folder,x))) \\\n",
    "                    for x in os.listdir(folder) if x[0]!='.']:\n",
    "    inptfldr = os.path.join(folder,fldr)\n",
    "    try:\n",
    "        # determine if raw data for each domain size is present at the desired temporal resolution.\n",
    "        fn = get_fn(inptfldr,fn1 = 'tippos_per_c_001',fn2 = 'tippos_per_001')\n",
    "        found=True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        found=False\n",
    "    if found:\n",
    "        directory = os.path.join(inptfldr,fn)\n",
    "        log_folder_parquet=f'{inptfldr}/positions'\n",
    "        input_dir = directory\n",
    "        print(input_dir)\n",
    "    #     print(os.listdir())\n",
    "        assert os.path.exists(input_dir)\n",
    "        if use_save_df_to_parquet_by or not os.path.exists(log_folder_parquet)\\\n",
    "                                    or not os.path.exists(os.path.join(log_folder_parquet,'trial_num=0')):\n",
    "            #parse each present folder to parquet, as before.\n",
    "            df_log=parse_fortranic_tip_pos(input_dir)\n",
    "            assert df_log.shape[0]>0\n",
    "            #partition df_log into a folder of tip logs\n",
    "            save_df_to_parquet_by(df_log,log_folder_parquet,by='trial_num',compression='snappy',index=None)\n",
    "            print(f\"saved to spiral tip positions to {log_folder_parquet=}\")\n",
    "    #         #determine width and height of the computational domain input the discretization\n",
    "    #         width,height=df_log.describe().loc['max'][['x','y']].values.T\n",
    "            del df_log\n",
    "        width,height = [eval(x) for x in fldr.split('x')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336020f",
   "metadata": {},
   "source": [
    "# compute $\\tau$ vs. $A$ for both full models conditioned on $N_0=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317c04c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:25:52.718Z"
    }
   },
   "outputs": [],
   "source": [
    "#for each full model\n",
    "DS = 0.025 #cm/pixel\n",
    "folder = '/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/'\n",
    "# folder = '/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_lr/'\n",
    "A_lst = []\n",
    "tau_lst = []\n",
    "Delta_tau_lst = []\n",
    "print(f\"tau vs. A for Fenton-Karma model:\")\n",
    "for fldr,fn_lst in [(x,os.listdir(os.path.join(folder,x))) \\\n",
    "                    for x in sorted(os.listdir(folder)) if x[0]!='.']:\n",
    "    width,height = [eval(x) for x in fldr.split('x')]\n",
    "    area = np.around(DS**2 * width * height,7)\n",
    "    inptfldr = os.path.join(folder,fldr)\n",
    "    log_folder_parquet = f'{inptfldr}/positions'\n",
    "    #compute tau(N0=2) for each domain size present. record.\n",
    "    try:\n",
    "        t_lst,nv_lst = gener_conditional_termination_times(log_folder_parquet,N0=2,seed=42,nv_once_every=10,\n",
    "                                            constrain_to_N0=True,return_particle_numbers=False)#,**kwargs)\n",
    "    except FileNotFoundError as e:\n",
    "        t_lst = None\n",
    "        nv_lst = None\n",
    "    if t_lst:\n",
    "        print(area, len(t_lst),log_folder_parquet)\n",
    "        #print summary stats for distribution of termination times\n",
    "        tau_values = np.array(t_lst)\n",
    "        meant,Delta_meant,num_obs,p_normal=comp_mean_bootstrap_uncertainty(tau_values)\n",
    "        print(f\"- {meant=},{Delta_meant=},{num_obs=},{p_normal=}\")\n",
    "        median = np.median(tau_values)\n",
    "        q25 = np.quantile(tau_values,0.25)\n",
    "        q75 = np.quantile(tau_values,0.75)\n",
    "        print(f\"- median: {median} (IQR: {q25}-{q75}, N={tau_values.shape[0]})\")\n",
    "        #record\n",
    "        A_lst.append(area)\n",
    "        tau_lst.append(meant)\n",
    "        Delta_tau_lst.append(Delta_meant)\n",
    "A_values_fk = np.array(A_lst); del A_lst\n",
    "tau_values_fk = np.array(tau_lst)*1e-3; del tau_lst\n",
    "Delta_tau_values_fk = np.array(Delta_tau_lst)*1e-3; del Delta_tau_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89d3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc83207",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:25:58.905Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONE: verify they're all different averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f654df3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:25:59.857Z"
    }
   },
   "outputs": [],
   "source": [
    "#for each full model\n",
    "DS = 0.025 #cm/pixel\n",
    "# folder = '/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_fk/'\n",
    "folder = '/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_lr/'\n",
    "A_lst = []\n",
    "tau_lst = []\n",
    "Delta_tau_lst = []\n",
    "print(f\"tau vs. A for Fenton-Karma model:\")\n",
    "for fldr,fn_lst in [(x,os.listdir(os.path.join(folder,x))) \\\n",
    "                    for x in sorted(os.listdir(folder)) if x[0]!='.']:\n",
    "    width,height = [eval(x) for x in fldr.split('x')]\n",
    "    area = np.around(DS**2 * width * height,7)\n",
    "    inptfldr = os.path.join(folder,fldr)\n",
    "    log_folder_parquet = f'{inptfldr}/positions'\n",
    "    #compute tau(N0=2) for each domain size present. record.\n",
    "    try:\n",
    "        t_lst,nv_lst = gener_conditional_termination_times(log_folder_parquet,N0=2,seed=42,nv_once_every=10,\n",
    "                                            constrain_to_N0=True,return_particle_numbers=False)#,**kwargs)\n",
    "    except FileNotFoundError as e:\n",
    "        t_lst = None\n",
    "        nv_lst = None\n",
    "    if t_lst:\n",
    "        print(area, len(t_lst),log_folder_parquet)\n",
    "        #print summary stats for distribution of termination times\n",
    "        tau_values = np.array(t_lst)\n",
    "        meant,Delta_meant,num_obs,p_normal=comp_mean_bootstrap_uncertainty(tau_values)\n",
    "        print(f\"- {meant=},{Delta_meant=},{num_obs=},{p_normal=}\")\n",
    "        median = np.median(tau_values)\n",
    "        q25 = np.quantile(tau_values,0.25)\n",
    "        q75 = np.quantile(tau_values,0.75)\n",
    "        print(f\"- median: {median} (IQR: {q25}-{q75}, N={tau_values.shape[0]})\")\n",
    "        #record\n",
    "        A_lst.append(area)\n",
    "        tau_lst.append(meant)\n",
    "        Delta_tau_lst.append(Delta_meant)\n",
    "A_values_lr = np.array(A_lst); del A_lst\n",
    "tau_values_lr = np.array(tau_lst)*1e-3; del tau_lst\n",
    "Delta_tau_values_lr = np.array(Delta_tau_lst)*1e-3; del Delta_tau_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d54de7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:00.757Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONE: verify they're all different averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4516c6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:01.105Z"
    }
   },
   "outputs": [],
   "source": [
    "# # load tip position data\n",
    "# trial_num = 0\n",
    "# log_folder_parquet = '/Users/timothytyree/Documents/GitHub/care/notebooks/Data/from_wjr/positions_lr/200x200/positions/'\n",
    "# g=load_parquet_by_trial_num(trial_num=trial_num,folder_parquet=log_folder_parquet)\n",
    "# n_series = g.groupby(by='t')['n'].min()\n",
    "# t0_values = n_series[n_series==N0].index.values\n",
    "# tf = n_series.index.values.max()\n",
    "# t_values = tf-t0_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531cb26",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:01.497Z"
    }
   },
   "outputs": [],
   "source": [
    "# t_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34f27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd6e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f5e687a",
   "metadata": {},
   "source": [
    "# recall particle model mean termination times conditioned on $N_0=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1918e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:03.325Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot against particle model \n",
    "# # taul_dir = f\"{nb_dir}/data/osg_output/run_46_tau_vs_L.csv\" #(run #46, N=2000 sims per data point)\n",
    "# # taul_dir = f\"{nb_dir}/data/osg_output/run_47_tau_vs_L.csv\" #(run #47, N=~2000 sims per data point)\n",
    "# taul_dir = f\"{nb_dir}/data/osg_output/run_48_tau_vs_L.csv\" #(run #48, N=~2000 sims per data point)\n",
    "df_taul = pd.read_csv(taul_dir)\n",
    "df_taul['A'] = df_taul['L'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0425ad1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:03.804Z"
    }
   },
   "outputs": [],
   "source": [
    "figsize=(3,4)\n",
    "fontsize=15\n",
    "alpha=0.7\n",
    "# max_num_points_to_show = 3\n",
    "max_num_points_to_show = 7\n",
    "fig,ax=plt.subplots(figsize=figsize)\n",
    "# color_lst = ['C0','C1']\n",
    "color_lst = ['k','r']\n",
    "mxpts=int(max_num_points_to_show)\n",
    "#plot particle model results\n",
    "for i,(D,g) in enumerate(df_taul.groupby('D')):\n",
    "    color = f\"C{i}\"\n",
    "    color = color_lst[i]\n",
    "#     g.plot.scatter(x='A',y='tau',color=color,ax=ax,marker='+')\n",
    "    ax.errorbar(g['A'][:mxpts], g['tau'][:mxpts], yerr=g['Delta_tau'][:mxpts],color=color,marker='+')\n",
    "    \n",
    "#plot full model results\n",
    "ax.errorbar(A_values_fk[:mxpts], tau_values_fk[:mxpts], yerr=Delta_tau_values_fk[:mxpts],color='C0')#,label='Fenton-Karma')#,marker='+')\n",
    "ax.errorbar(A_values_lr[:mxpts], tau_values_lr[:mxpts], yerr=Delta_tau_values_lr[:mxpts],color='C1')#,label='Luo-Rudy')#,marker='+')\n",
    "ax.scatter(A_values_fk[:mxpts],tau_values_fk[:mxpts],c='C0',alpha=alpha,label='Fenton-Karma')\n",
    "ax.scatter(A_values_lr[:mxpts],tau_values_lr[:mxpts],c='C1',alpha=alpha,label='Luo-Rudy')\n",
    "\n",
    "# ax.scatter(xfk[:mxpts],yfk[:mxpts],c='C0',alpha=alpha,label='Fenton-Karma')\n",
    "# ax.scatter(xlr[:mxpts],ylr[:mxpts],c='C1',alpha=alpha,label='Luo-Rudy')\n",
    "#plot the simulated annealing fits\n",
    "# # ax.scatter(xfk,yfk_pred,c='k',alpha=alpha,marker='+')#,label='Fenton-Karma')\n",
    "# # ax.scatter(xlr,ylr_pred,c='k',alpha=alpha,marker='+')#,label='Luo-Rudy')\n",
    "# # ax.scatter([A_fk],[tau_fk],marker='o',edgecolor='k',c='C0',#'k',\n",
    "# #            alpha=1,label='Fenton-Karma')\n",
    "# # ax.scatter([A_lr],[tau_lr],marker='o',edgecolor='k',c='C1',#c='k',\n",
    "# #            alpha=1,label='Luo-Rudy')\n",
    "# # xv=np.linspace(0,40,10)\n",
    "# # ax.plot(xv,taubar0fk*np.exp(xv/A0fk),':',color='gray')#,'k--',lw=2)\n",
    "# # xv=np.linspace(0,80,10)\n",
    "# # ax.plot(xv,taubar0lr*np.exp(xv/A0lr),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,40,10)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foofk(xv),':',color='gray')#,'k--',lw=2)\n",
    "# xv=np.linspace(10,180,10)\n",
    "# ax.plot(xv,tau_pred_foolr(xv),':',color='gray')#,'k--',lw=2)\n",
    "ax.set_xticks([0,50,100,150])\n",
    "# format_plot(ax=ax,xlabel=r'$a$ (cm$^2/$s)',ylabel=r'$\\tau$  (s)',fontsize=16)\n",
    "format_plot(ax=ax,xlabel=r'$A$ (cm$^2$)',ylabel=r'$\\langle\\tau(N=2)\\rangle$  (s)',fontsize=16)\n",
    "# ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='both')\n",
    "ax.tick_params(top=True, right=True,direction='in',which='minor',length=3)\n",
    "# ax.set_ylim([1e-2,100])\n",
    "# # AddLegend(ax=ax,xy=(-0.12,0.240),fontsize=fontsize-3)\n",
    "leg = ax.legend(loc='upper right',fontsize=fontsize-2)#,alpha=1.)#,frameon=False,ncol=1)\n",
    "# leg.set_alpha(1.)\n",
    "# , edgecolor=\"black\")\n",
    "leg.get_frame().set_alpha(None)\n",
    "# leg.get_frame().set_facecolor((1, 1, 1, 1))\n",
    "# leg = ax.legend(loc='lower right',fontsize=fontsize-3.,frameon=False,ncol=1)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "# ax.set_ylim([0.31392551380514033, 4e9])\n",
    "# ax.set_xlim([0.,65])\n",
    "ax.set_xlim([0.,105])\n",
    "ax.set_ylim([0.13850549065122997,16000])\n",
    "# ax.set_xlim([0.,100])\n",
    "# ax.set_ylim([0.13850549065122997,12000])\n",
    "# AddLege nd(ax=ax,xy=(0.05,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.7,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.4,0.990),fontsize=fontsize-3)\n",
    "# AddLegend(ax=ax,xy=(0.1,0.25),fontsize=fontsize-3)\n",
    "# ax.tick_params(axis='y', which='minor', left=True)\n",
    "ax.set_xticks([0,25,50,75,100])\n",
    "# plt.minorticks_on()\n",
    "plt.show()\n",
    "ax.axis()\n",
    "#og src: http://localhost:8888/notebooks/Simulating%20the%20paired%20birth%20death%20process.ipynb\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea89f13",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:04.364Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(A_values_fk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a7fd2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:04.804Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(A_values_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bf9c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:05.096Z"
    }
   },
   "outputs": [],
   "source": [
    "#GOAL: plot tau vs. A for full and particle model at same A values conditioned on N0=2.\n",
    "#DONE: visualize tau(N0=2) for the full models\n",
    "#DONE: truncate/crop view to look as exponential as possible\n",
    "#DONT: add in an extra 2 data points for the FK model. bc they're not tau | N0=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90a5ca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T19:26:05.461Z"
    }
   },
   "outputs": [],
   "source": [
    "beep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf8cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9bc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
